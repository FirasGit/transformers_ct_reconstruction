{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment as current working directory\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import ConcatDataset\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "from datasets.data_module import SimpleDataModule\n",
    "from datasets import LIDCDataset\n",
    "from vqgan.model import VQVAE, VQGAN, VAE, VAEGAN\n",
    "\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This Notebook has to be run three times using different settings for the parameters below.\n",
    "# First Setting: Choose USE_2D = False\n",
    "# Second Setting: Choose USE_2D = True and PROJECTION_PLANE = 'lat'\n",
    "# Third Setting: Choose USE_2D = True and PROJECTION_PLANE = 'ap'\n",
    "\n",
    "USE_2D = False # True or False\n",
    "PROJECTION_PLANE = 'lat' # 'ap' or 'lat'\n",
    "PATH_TO_PREPROCESSED_DATA = '' # Replace this with the folder containing the preprocessed LIDC-IDRI dataset (i.e., <PATH_TO_PREPROCESSED_DATA>)\n",
    "BEST_VQ_GAN_CKPT_2D = '' # Replace this with the best VQ-GAN checkpoint for the 2D model\n",
    "BEST_VQ_GAN_CKPT_3D = '' # Replace this with the best VQ-GAN checkpoint for the 3D model\n",
    "STORAGE_DIR = '' # Replace this with the desired path for storing the indices (e.g. /data/lidc_indices/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir_path):\n",
    "\tif not os.path.exists(dir_path):\n",
    "\t\tos.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = [0] if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_2D == True:\n",
    "    lidc_dataset_train = LIDCDataset(\n",
    "        root_dir=PATH_TO_PREPROCESSED_DATA, augmentation=False, projection=True, split='train', projection_plane=PROJECTION_PLANE)\n",
    "\n",
    "    lidc_dataset_val = LIDCDataset(\n",
    "        root_dir=PATH_TO_PREPROCESSED_DATA, augmentation=False, projection=True, split='val', projection_plane=PROJECTION_PLANE)\n",
    "\n",
    "    lidc_dataset_test = LIDCDataset(\n",
    "        root_dir=PATH_TO_PREPROCESSED_DATA, augmentation=False, projection=True, split='test', projection_plane=PROJECTION_PLANE)\n",
    "\n",
    "    dm = SimpleDataModule(\n",
    "        ds_train=lidc_dataset_train,\n",
    "        ds_val=lidc_dataset_val,\n",
    "        ds_test=lidc_dataset_test,\n",
    "        batch_size=1,\n",
    "        num_workers=1,\n",
    "        pin_memory=True\n",
    "    )\n",
    "else:\n",
    "    lidc_dataset_train = LIDCDataset(\n",
    "        root_dir=PATH_TO_PREPROCESSED_DATA, augmentation=False, split='train')\n",
    "\n",
    "    lidc_dataset_val = LIDCDataset(\n",
    "        root_dir=PATH_TO_PREPROCESSED_DATA, augmentation=False, split='val')\n",
    "\n",
    "    lidc_dataset_test = LIDCDataset(\n",
    "        root_dir=PATH_TO_PREPROCESSED_DATA, augmentation=False, split='test')\n",
    "\n",
    "    dm = SimpleDataModule(\n",
    "        ds_train=lidc_dataset_train,\n",
    "        ds_val=lidc_dataset_val,\n",
    "        ds_test=lidc_dataset_test,\n",
    "        batch_size=1,\n",
    "        num_workers=30,\n",
    "        pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_2D:\n",
    "    model = VQGAN(\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        emb_channels=512,\n",
    "        num_embeddings=8192,\n",
    "        spatial_dims=2,\n",
    "        hid_chs=[64, 128, 256, 512],\n",
    "        kernel_sizes=[3,  3, 3, 3],\n",
    "        strides=[1, 2, 2, 2],\n",
    "        embedding_loss_weight=1,\n",
    "        beta=1,\n",
    "        pixel_loss=torch.nn.L1Loss,\n",
    "        deep_supervision=1,\n",
    "        use_attention='none',\n",
    "        sample_every_n_steps=50,\n",
    "    )\n",
    "\n",
    "    model.load_pretrained(BEST_VQ_GAN_CKPT_2D)\n",
    "else:\n",
    "    model = VQGAN(\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    emb_channels=256,\n",
    "    num_embeddings=8192,\n",
    "    spatial_dims=3,\n",
    "    hid_chs=[32, 64,  128, 256],\n",
    "    kernel_sizes=[3,  3,   3, 3],\n",
    "    strides=[1,  2,   2, 2],\n",
    "    embedding_loss_weight=1,\n",
    "    beta=1,\n",
    "    pixel_loss=torch.nn.L1Loss,\n",
    "    deep_supervision=0,\n",
    "    use_attention='none',\n",
    "    norm_name=(\"GROUP\", {'num_groups': 4, \"affine\": True}),\n",
    "    sample_every_n_steps=200,\n",
    "    )\n",
    "\n",
    "    model.load_pretrained(BEST_VQ_GAN_CKPT_3D)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get next element of dataloader\n",
    "test_sample = next(iter(dm.test_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLICE_NUM = 60\n",
    "\n",
    "if USE_2D:\n",
    "\tplt.imshow(test_sample['source'][0][0], cmap='gray')\n",
    "\tplt.axis('off')\n",
    "else:\n",
    "\tplt.imshow(test_sample['source'][0][0][SLICE_NUM], cmap='gray')\n",
    "\tplt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sample = model(test_sample['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_2D:\n",
    "\tplt.imshow(out_sample[0][0][0].detach().cpu(), cmap='gray')\n",
    "\tplt.axis('off')\n",
    "else:\n",
    "\tplt.imshow(out_sample[0][0][0][SLICE_NUM].detach().cpu(), cmap='gray')\n",
    "\tplt.axis('off')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, embedding_shape = model.vqvae.encode_to_indices(test_sample['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sample_2 = model.vqvae.decode_from_indices(indices, embedding_shape)\n",
    "\n",
    "if USE_2D:\n",
    "\tplt.imshow(out_sample_2[0][0].detach().cpu(), cmap='gray')\n",
    "\tplt.axis('off')\n",
    "else:\n",
    "\tplt.imshow(out_sample_2[0][0][SLICE_NUM].detach().cpu(), cmap='gray')\n",
    "\tplt.axis('off')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert all images to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get next element of dataloader\n",
    "storage_dir = STORAGE_DIR \n",
    "train_path = os.path.join(storage_dir, 'train') \n",
    "val_path = os.path.join(storage_dir, 'val')\n",
    "test_path = os.path.join(storage_dir, 'test')\n",
    "create_dir(train_path)\n",
    "create_dir(val_path)\n",
    "    \n",
    "for split in [[train_path, dm.train_dataloader()], [val_path, dm.val_dataloader()], [test_path, dm.test_dataloader()]]:\n",
    "\tfor sample in tqdm(split[1]):\n",
    "\t\tindices, embedding_shape = model.vqvae.encode_to_indices(sample['source'])\n",
    "\t\tfile_name = sample['file_name'][0].split('/')[-2] \n",
    "\t\tindices_np = indices.detach().cpu().numpy()\n",
    "\t\tfolder_path = os.path.join(split[0], file_name)\n",
    "\t\tcreate_dir(folder_path)\n",
    "\t\tif USE_2D:\n",
    "\t\t\tnp.save(os.path.join(folder_path, f'{PROJECTION_PLANE}.npy'), indices_np)\n",
    "\t\telse:\n",
    "\t\t\tnp.save(os.path.join(folder_path, 'CT.npy'), indices_np)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medicaldiffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "489df2b2e73de1ddceac97fbee82a53bd2a027d2efa7299f0a23dfd27bb8968f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
